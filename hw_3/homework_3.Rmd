---
title: "Homework 3"
author: "Joe Davis"
date: "3/25/2022"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(smss) #data from smss book
library(alr4) #data from alr book
library(tidyverse) #because always, even if I probably don't need it for this one.
library(broom) #to return model summaries as tibbles for easier use in sub questions and glance function


```
### Formatting Note ###

Being that this homework has many more sub questions than previous ones, I'm going to move the R chunks to right after the sub questions versus doing the massive wall of code outputs. I assume this will be a welcome change compared to previous efforts.

### Question 1 ###

*For recent data in Jacksonville, Florida, on y = selling price of home (in dollars), x1 = size of home (in square feet), and x2 = lot size (in square feet), the prediction equation is ŷ = −10,536 + 53.8x1 + 2.84x2.*

*-A) A particular home of 1240 square feet on a lot of 18,000 square feet sold for $145,000. Find the predicted selling price and the residual, and interpret.*

```{r, include = TRUE}
##Question A math work
  #observed price of home sold
house_price_actual <- 145000

  #predicted price of home sold
house_price_prediction <-  -10536 + (53.8*1240) + (2.84*18000)

  #get residual
  house_price_residual <- house_price_prediction - house_price_actual
  
```

For the predicted sale price of the house using 1240 sq ft for x1 and 18000 square ft for x2, I found `r format(house_price_prediction, scientific = FALSE)` and the residual when comparing to the observed sale price of $145,000 is `r format(house_price_residual, scientific = FALSE)`. This means that our model was under-predicted the observed value of home sales by a pretty substantial amount. I suspect this is due to the model failing to account for other variables that explain other factors in the final sale price of a home. Distance to downtown, school districts, or other variables likely play a part in describing the distance between our predicted and actual values.


*-B) For fixed lot size, how much is the house selling price predicted to increase for each square-foot increase in home size? Why?*

For each square foot the predicted home price increase is $53.80. This is because the slope for that variable is in change for each increment of the variable on the outcome variable controlling for the other variables in the equation for this particular type of multiple regression. Since lot sized is fixed here, we wind up with the proof of that concept as the intercept and x2 terms would remain constant while only the 53.8x term would change. 

*-C) According to this prediction equation, for fixed home size, how much would lot size need to increase to have the same impact as a one-square-foot increase in home size?*

If we divide the x1 slope by the x2 slope, we will get the square footage the lot size would have to increase by in order to equal the incremental change from a square foot increase of the home size. Calculated as `r round(digits = 2, sum(53.8 / 2.84)) ` square feet rounding to the same digits as that slope term.   


### Question 2 ##
*The data file concerns salary and other characteristics of all faculty in a small Midwestern college collected in the early 1980s for presentation in legal proceedings for which discrimination against women in salary was at issue. All persons in the data hold tenured or tenure track positions; temporary faculty are not included. The variables include degree, a factor with levels PhD and MS; rank, a factor with levels Asst, Assoc, and Prof; sex, a factor with levels Male and Female; Year, years in current rank; ysdeg, years since highest degree, and salary, academic year salary in dollars.*
```{r include = TRUE}
##load the data for the following questions
salary_dat <- as_tibble(salary)
```
*-A) Test the hypothesis that the mean salary for men and women is the same, without regard to any other variable but sex. Explain your findings.*
```{r Q2a}
## Ho: men_salary = women_salary. Ha: men_salary =/= women_salary, telling it to fit intercept explicitly in the lm function of means with a factor variable
salary_lm_a <- lm(salary ~ 1 + sex, data = salary_dat)

#use confint to find 95% for sex parameter
tidy(salary_lm_a)
confint(salary_lm_a)
```

By fitting a model for the data and getting a fit for the intercept, which will represent the `Male` salary data and the fit for the `Female` survey data we can compare their means and get p-value for the `Female` salary data. With the estimated level for the Female variable at -3340 of the intercept/Male data, we can say that we estimate that men are being paid more by just over 3300 dollars. We are confident of this at the .1 level for the two sided test, and looking at the `confint` range it seems likely that people in the `Female` dataset at this school were getting paid lower looking at the `sex` variable alone.

*-B) Run a multiple linear regression with salary as the outcome variable and everything else as predictors, including sex. Assuming no interactions between sex and the other predictors, obtain a 95% confidence interval for the difference in salary between males and females.*

```{r Q2b Solving}
## Do the linear model for all of the variables. Year is lowercase unlike Q
salary_lm_b <- lm(salary ~ sex + degree + rank + year + ysdeg, data = salary_dat)

#look at the model summary. point estimate has flipped to positive. hmm.
summary(salary_lm_b)

#confint for the sexFemale
confint(salary_lm_b, "sexFemale", level = .95)


```
Only the next question asks for the interpretation, but I'll put that question for this variable examination here. The p-value for the `sexFemale` variable is above thresholds for significance, so the confidence in our interval of difference is pretty low. I'm suspicious of it. The positive point estimate as well as the range on the interval, however, would suggest that holding all else equal that `sexFemale` is associated with a *higher* salary.


*-C) Interpret your finding for each predictor variable; discuss (a) statistical significance, (b) interpretation of the coefficient / slope in relation to the outcome variable and other variables*

The `rank` variable and its levels of Professor and Associate Professor are significant all the way up at the functional 0 level in relation to the Assistant Professor level. The coefficients show that each level of `rank` that a Professor climbs is associate with a little bit more than five thousand dollar increase in our predicted salary formula. 

The `year` someone has been in the current rank variable level is significant at the same level code, but with a much lower slope when compared to the coefficient leap between ranks suggesting that getting across the cataegory threshold in `rank` is more important than time served for salary. Between 10 and 12 years of service equates to the change in a `rank` level. 

The `degreePhD` coefficient is not significant with its `p` of .180, but it does have a positive value of 1388.61. I was a little surprised by this one, but perhaps the rank variable or something else picks up the effects of higher education? I would have assumed it would have been a little bit higher from the very little I know about general education union contracts. But hey, it was the 1980's who knows what was going on. 

The `ysdeg` variable has a slightly negative slope, but it is just beyond the .1 level of significance. The only thing I could think of that might explain the slope generally, though I'm not certain it's really helping the model all that much given the other categories, would be to adjust some of the longer serving professors who only have masters degrees downwards slightly, and since we don't have an interaction between the `ysdeg` and `degree` level it's messy enough to not be at a statistically significant level. 

*-D) Change the baseline category for the rank variable. Interpret the coefficients related to rank again.*
My lack of clear reading of the instructions on the variables almost got me again, just like the quizzes! However, I realized just in time that the `rank` variable has three levels and I was thinking about the `degree` variable in my head for some reason. Phew, okay I'm going to relevel the `rank` variable on Associate Professor and check it out.  

```{r Solve Q2d}
#relevel the rank variable, run the model again
salary_dat$rank <- relevel(salary_dat$rank, ref = "Assoc")

#new model
salary_lm_d <- lm(salary ~ rank + sex + degree + year + ysdeg, data = salary_dat)

#summarizing the model
summary(salary_lm_d)

```
Now that the middle `rank` is the reference category, the coefficients have shifted so that the roughly five thousand dollar increase we predict as you go up the scale is now seen in the in the -5292.36 coefficient value of stepping down to the `Asst` level, and the same 5826.4 increase that we saw in the last model going from `Assoc` to `Prof`.


*-E) Finkelstein (1980), in a discussion of the use of regression in discrimination cases, wrote, “[a] variable may reflect a position or status bestowed by the employer, in which case if there is discrimination in the award of the position or status, the variable may be ‘tainted.’ ” Thus, for example, if discrimination is at work in promotion of faculty to higher ranks, using rank to adjust salaries before comparing the sexes may not be acceptable to the courts. Exclude the variable rank, refit, and summarize how your findings changed, if they did.*

```{r Solving Q2e}

#same as salary_lm_b but removing rank.
salary_lm_e <- lm(salary ~ sex + degree + year + ysdeg, data = salary_dat)

#summary of model
summary(salary_lm_e)
confint(salary_lm_e, "sexFemale", level = .95)

```
The `sexFemale` coefficient is back to being a negative value relative to the `Male` level in our intercept. The `p` value is above significance thresholds, however. 

Outside of the main point of the question, I'm a little stumped on why the `degreePhD` coefficient is negative relative to the `Masters`level in the intercept besides perhaps small sample size messiness. The`ysdeg` variable looks like it picks up some of the descriptive power that the rank variable left behind as we would expect. I would have assumed some of the same for PhD. I'm sure my self-narration of something that's potentially indicating the whole function is wrong and therefore all of my answers will be amusing after I see the actual answers.


*-F) Everyone in this dataset was hired the year they earned their highest degree. It is also known that a new Dean was appointed 15 years ago, and everyone in the dataset who earned their highest degree 15 years ago or less than that has been hired by the new Dean. Some people have argued that the new Dean has been making offers that are a lot more generous to newly hired faculty than the previous one and that this might explain some of the variation in Salary.Create a new variable that would allow you to test this hypothesis and run another multiple regression model to test this. Select variables carefully to make sure there is no multicollinearity. Explain why multicollinearity would be a concern in this case and how you avoided it. Do you find support for the hypothesis that the people hired by the new Dean are making higher than those that were not?*

```{r solve Q2f}
#relevel rank
salary_dat$rank <- relevel(salary_dat$rank, ref = "Asst")

#make a dummy variable for years since degree less than 15 to note as hired by new_dean.
salary_dat <- salary_dat %>%
  mutate(new_dean = case_when(
    ysdeg <= 15 ~ 1,
    ysdeg > 15 ~ 0
  ))



#the model
salary_lm_f <- lm(salary ~ year + sex + degree + rank + new_dean + new_dean*year, data = salary_dat)

#summary of model
summary(salary_lm_f)
confint(salary_lm_f, "new_dean", level = .95)
```

Multicollinearity is a concern in this question because the years since highest degree and the new variable `new_dean` would be measuring similar things and therefore not clearing the additivity consideration. The variable `year` in rank would also potentially be overlapping with the new_dean dummy variable, as the same pre and post 15 year threshold would potentially be seen there. To deal with this, I added an interaction effect with the   `year` in rank variable and the `new_dean` variable, and after looking at the adjusted R-squared for the model with and without `ysdeg` and a `ysdeg*new_dean` and even the monstrosity of `ysdeg*years*new_dean`, dropping `ysdeg` made for a better fit. This makes sense as the variable itself lost significance when the `rank` level was included previously, so dropping that term and adding the interaction effect should deal with the multicollinearity. I do find support at the .05 significance level that being hired by the new Dean are making a higher wage than those that were not. The estimated dollar amount of the effect is 3789.08, and I printined the 95% confidence interval above as well.

### Question 3 ###

*-A) Using the house.selling.price data, run and report regression results modeling y = selling price (in dollars) in terms of size of home (in square feet) and whether the home is new (1 = yes; 0 = no). (In other words, price is the outcome variable and size and new are the explanatory variables.)*

*-B) Report and interpret the prediction equation, and form separate equations relating selling price to size for new and for not new homes. In particular, for each variable; discuss statistical significance and interpret the meaning of the coefficient.*

*-C) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.*

*-D) Fit another model, this time with an interaction term allowing interaction between size and new, and report the regression results*

*-E) Report the lines relating the predicted selling price to the size for homes that are (i) new, (ii) not new.*

*-F) Find the predicted selling price for a home of 3000 square feet that is (i) new, (ii) not new.*

*-G) Find the predicted selling price for a home of 1500 square feet that is (i) new, (ii) not new. Comparing to (F), explain how the difference in predicted selling prices changes as the size of home increases.*

*-H) Do you think the model with interaction or the one without it represents the relationship of size and new to the outcome price? What makes you prefer one model over another?*

